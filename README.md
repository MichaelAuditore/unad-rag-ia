**UNAD RAG AI â€“ Asistente AcadÃ©mico Inteligente con IA Generativa Local**

Este proyecto implementa un **prototipo funcional (TRL 5â€“6)** de un asistente inteligente para la **Universidad Nacional Abierta y a Distancia (UNAD)**, desarrollado con tecnologÃ­as **de cÃ³digo abierto** y ejecutado **localmente**, sin dependencia de servicios en la nube.

El sistema integra un modelo **RAG (Retrieval-Augmented Generation)** con **memoria conversacional** y una **interfaz web interactiva**, capaz de responder preguntas sobre **programas acadÃ©micos, polÃ­ticas de gratuidad y reglamentos institucionales**.

El agente estÃ¡ construido sobre el ecosistema **LangChain + Ollama + ChromaDB**, empleando modelos locales como **Llama 3** o **Mistral**, y se ejecuta completamente en entorno **Python** mediante **Gradio** como interfaz grÃ¡fica.

ğŸ”¹ **CaracterÃ­sticas principales:**

* RecuperaciÃ³n inteligente de informaciÃ³n institucional (RAG local).
* Memoria conversacional para mantener el contexto del diÃ¡logo.
* IntegraciÃ³n con documentos PDF, TXT y fuentes web de la UNAD.
* Arquitectura modular y extensible.
* CÃ³digo abierto y ejecutable sin conexiÃ³n a Internet.

ğŸ”¹ **TecnologÃ­as utilizadas:**

* ğŸ Python 3.10+
* ğŸ§© LangChain
* ğŸ¤– Ollama (modelos Llama 3 / Mistral)
* ğŸ§  ChromaDB
* ğŸ’¬ Gradio (interfaz)
* ğŸ§¾ Sentence Transformers

---

## âš™ï¸ Requisitos previos

Antes de comenzar, asegÃºrate de tener instalado:

* ğŸ³ **Docker** y **Docker Compose**
* ğŸ’¾ Al menos **6â€“8 GB de RAM**
* ğŸ“ Espacio libre de **5â€“10 GB** (segÃºn el modelo elegido)

---

## ğŸš€ InstalaciÃ³n y ejecuciÃ³n

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone https://github.com/tuusuario/unad-rag-agent
cd unad-rag-agent
```

### 2ï¸âƒ£ Agregar documentos de conocimiento

Coloca tus archivos `.pdf` o `.txt` dentro del directorio `knowledge/`.
Por ejemplo:

```
knowledge/
â”œâ”€â”€ programas_academicos.pdf
â”œâ”€â”€ reglamento_estudiantil.pdf
â””â”€â”€ politicas_institucionales.txt
```

### 3ï¸âƒ£ Configurar variables de entorno

Copia el archivo `.env.example` y renÃ³mbralo a `.env`:

```bash
cp .env.example .env
```

Puedes ajustar el modelo a utilizar:

```bash
OLLAMA_MODEL=phi3:3.8b
```

### 4ï¸âƒ£ Iniciar el asistente

Levanta todo el sistema (Ollama + App):

```bash
docker compose up --build
```

Una vez iniciado, abre tu navegador en:
ğŸ‘‰ **[http://localhost:7860](http://localhost:7860)**

---

## ğŸ§  Cambiar modelo LLM

Puedes modificar el modelo en `.env` o en `docker-compose.yml`.
Los mÃ¡s recomendados son:

| Modelo       | TamaÃ±o  | CaracterÃ­sticas                                      |
| ------------ | ------- | ---------------------------------------------------- |
| `mistral:7b`   | ~2 GB   | Muy rÃ¡pido, eficiente, ideal para respuestas simples |
| `phi3:3.8b`  | ~3.8 GB | Excelente en espaÃ±ol y razonamiento                  |
| `mistral:7b` | ~7 GB   | MÃ¡s potente, buena comprensiÃ³n contextual            |
| `llama3:8b`  | ~8 GB   | Buen equilibrio entre velocidad y calidad            |

DespuÃ©s de cambiar el modelo, simplemente ejecuta:

```bash
docker compose restart unad_rag
```

---

## ğŸ”„ Reconstruir la base de conocimiento

Si agregas nuevos documentos o deseas regenerar el Ã­ndice:

```bash
docker compose run unad_rag python app.py --reindex
```

Esto recrearÃ¡ la base vectorial (`db/chroma`).

---

## ğŸ§© Arquitectura interna

* **Gradio** â†’ Interfaz de chat web
* **LangChain** â†’ OrquestaciÃ³n RAG y memoria conversacional
* **Chroma** â†’ Almacenamiento vectorial
* **Ollama** â†’ Motor local de modelos open-source
* **SentenceTransformers** â†’ GeneraciÃ³n de embeddings

---

## ğŸ§ª Ejecutar localmente sin Docker

Si prefieres ejecutar directamente en tu mÃ¡quina:

```bash
pip install -r requirements.txt
python app.py --reindex
```

Y accede desde: [http://localhost:7860](http://localhost:7860)

---

## ğŸ§° Comandos Ãºtiles

| AcciÃ³n                    | Comando                                               |
| ------------------------- | ----------------------------------------------------- |
| Levantar todo             | `docker compose up --build`                           |
| Reconstruir base de datos | `docker compose run unad_rag python app.py --reindex` |
| Instalar modelo LLM       | `docker exec -it ollama ollama pull mistral:7b` |
| Instalar modelo embedding | `docker exec -it ollama ollama pull nomic-embed-text` |
| Cambiar modelo            | Edita `.env` y ejecuta `docker compose restart`       |
| Detener contenedores      | `docker compose down`                                 |

---

## ğŸ’¡ Consejos

* Guarda tus documentos organizados en `knowledge/`.
* Usa modelos pequeÃ±os si tu equipo tiene poca RAM.
* No necesitas conexiÃ³n a Internet despuÃ©s de descargar el modelo.

---

## ğŸ§¾ Licencia

Este proyecto es **open-source** y puede modificarse libremente con fines educativos y de investigaciÃ³n.

---

ğŸ“ **Nivel de madurez tecnolÃ³gica:** TRL 5â€“6 (validaciÃ³n de sistema completo en entorno relevante).

ğŸ“‚ **Licencia:** MIT

ğŸ‘¤ **Autores:** Miguel Ãngel Parada CaÃ±on, Tania Parrado Rojas

ğŸ« **Escuela:** ECBTI â€“ IngenierÃ­a de Sistemas â€“ UNAD
